# **************************************************************************
# *                                                                          *
# *                        å†œäº§å“å¸‚åœºæ•°æ®çˆ¬è™«                                 *
# *                                                                          *
# *                          ä½œè€…: xiaohai                                   *
# *                          ç‰ˆæœ¬: v1.0.0                                    *
# *                          æ—¥æœŸ: 2024-12-05                                *
# *                                                                          *
# *          åŠŸèƒ½:                                                           *
# *              - æ”¯æŒé€‰æ‹©æ€§çˆ¬å–æŒ‡å®šçœä»½æ•°æ®                                 *
# *              - æ”¯æŒCSV/JSONå¤šç§æ ¼å¼å¯¼å‡º                                  *
# *              - è‡ªåŠ¨ä¿å­˜å†å²æ•°æ®å¹¶å»é‡                                    *
# *              - æ”¯æŒæ–­ç‚¹ç»­ä¼ å’Œé”™è¯¯é‡è¯•
# *                                                                          *
# *          æ³¨æ„:                                                           *
# *              - ç”±äºæ•°æ®æ¥æºçš„é™åˆ¶ï¼Œéƒ¨åˆ†æ•°æ®å¯èƒ½æ— æ³•è·å–                 *
# *              - éƒ¨åˆ†æ•°æ®å¯èƒ½å­˜åœ¨ç¼ºå¤±æˆ–é”™è¯¯                                  *
# *              - è¯·ç¡®ä¿åœ¨åˆæ³•åˆè§„çš„å‰æä¸‹ä½¿ç”¨æœ¬ç¨‹åº                      *
# *                                                                          *
# *          è¿è¡Œ:                                                           *
# *              - å‘½ä»¤è¡Œè¿è¡Œ: python market_crawler.py                        *
# *              - é€‰æ‹©æ€§è¿è¡Œ: python market_crawler.py -p å¹¿ä¸œçœ                *
# *                                                                          *
# *          å¯¼å‡º:                                                           *
# *              - CSVæ ¼å¼: python market_crawler.py -p å¹¿ä¸œçœ -o data.csv     *
# *              - JSONæ ¼å¼: python market_crawler.py -p å¹¿ä¸œçœ -o data.json   *
# *             - Excelæ ¼å¼: python market_crawler.py -p å¹¿ä¸œçœ -o data.xlsx 
# *                                                                          *
# *          æ–‡ä»¶å¤¹æ ¼å¼:                                                       *
# *              - market_crawler                                             *
# *                  - 20xxxxxx                                               *
# *                      - å¸‚åœº1                                               *
# *                      - å¸‚åœº2                                               *
# *                      -...                                                 *
# *                  - 20xxxxxx                                               *
# *                      - å¸‚åœº1                                               *
# *                      - å¸‚åœº2                                               *
# *                      -...                                                 *
# *                  -...                                                   *
# *                  - merged                                                 *
# *                      - all_data_2024xxxx.csv                               *
# *                      - all_data_2024xxxx.json                              *
#                    - summary                                                *
# *                      - summary_2024xxxx.csv                                *
# *                      - summary_2024xxxx.json                              *
# *                      - summary_2024xxxx.csv                              *
# *                      - summary_2024xxxx.json                              *
# *                      -...                                                  *
# *              - market_crawler_log.txt                                       *
#                - market_crawler.py                                          *
# *                                                                          *
# *          æ–­ç‚¹ç»­ä¼ :                                                        *
# *              - ç¨‹åºä¼šè‡ªåŠ¨ä¿å­˜å†å²æ•°æ®ï¼Œä¸‹æ¬¡è¿è¡Œæ—¶ä¼šè‡ªåŠ¨åŠ è½½å¹¶å»é‡            *
# *              - è‹¥éœ€è¦é‡æ–°å¼€å§‹ï¼Œè¯·åˆ é™¤å†å²æ•°æ®æ–‡ä»¶                          *
# *                                                                          *
# **************************************************************************

import pkg_resources
import sys
import argparse
import subprocess

def check_and_install_packages():
    """æ£€æŸ¥å¹¶å®‰è£…æ‰€éœ€çš„åŒ…"""
    required_packages = {
        'requests': 'requests',
        'pandas': 'pandas',
        'beautifulsoup4': 'bs4',
        'urllib3': 'urllib3',
        'openpyxl': 'openpyxl',  # Excelæ”¯æŒ
        'lxml': 'lxml',          # XMLè§£æå™¨
        'chardet': 'chardet',    # å­—ç¬¦ç¼–ç æ£€æµ‹
        'tqdm': 'tqdm',          # è¿›åº¦æ¡
        'colorama': 'colorama'   # æ§åˆ¶å°é¢œè‰²
    }
    
    print("\n" + "="*50)
    print("æ£€æŸ¥å¹¶å®‰è£…ä¾èµ–åŒ…...")
    print("="*50)
    
    try:
        import colorama
        colorama.init()  # åˆå§‹åŒ–æ§åˆ¶å°é¢œè‰²
        success_mark = colorama.Fore.GREEN + "âœ“" + colorama.Style.RESET_ALL
        error_mark = colorama.Fore.RED + "âœ—" + colorama.Style.RESET_ALL
    except ImportError:
        success_mark = "âœ“"
        error_mark = "âœ—"
    
    all_success = True

    # ğŸ‘‡ é•œåƒæºé…ç½®ï¼ˆå¯æ›´æ¢ä¸ºå…¶ä»–æºï¼‰
    mirror_url = "https://mirrors.aliyun.com/pypi/simple/"
    trusted_hosts = [
        "files.pythonhosted.org",
        "pypi.org",
        "mirrors.aliyun.com"
    ]

    for package, import_name in required_packages.items():
        try:
            pkg_resources.require(package)
            print(f"{success_mark} {package:15} å·²å®‰è£…")
        except (pkg_resources.DistributionNotFound, pkg_resources.VersionConflict):
            print(f"{error_mark} {package:15} æœªå®‰è£…ï¼Œæ­£åœ¨å®‰è£…...")
            try:
                # ä½¿ç”¨ pip å®‰è£…åŒ…ï¼Œå¹¶æŒ‡å®šé•œåƒæºå’Œä¿¡ä»»ä¸»æœº
                subprocess.check_call([
                    sys.executable,
                    "-m",
                    "pip",
                    "install",
                    "--disable-pip-version-check",  # ç¦ç”¨pipç‰ˆæœ¬æ£€æŸ¥
                    "--no-cache-dir",               # ç¦ç”¨ç¼“å­˜
                    "-i", mirror_url,               # æŒ‡å®šé•œåƒæº
                    *[f"--trusted-host={host}" for host in trusted_hosts],  # æ·»åŠ ä¿¡ä»»çš„ä¸»æœº
                    package
                ], stdout=subprocess.DEVNULL)
                print(f"{success_mark} {package:15} å®‰è£…æˆåŠŸ")
            except subprocess.CalledProcessError as e:
                print(f"{error_mark} {package:15} å®‰è£…å¤±è´¥: {str(e)}")
                all_success = False
            except Exception as e:
                print(f"{error_mark} {package:15} å®‰è£…å‡ºé”™: {str(e)}")
                all_success = False
    
    print("\nä¾èµ–åŒ…æ£€æŸ¥" + ("å…¨éƒ¨å®Œæˆ" if all_success else "å­˜åœ¨é—®é¢˜"))
    print("="*50 + "\n")
    
    if not all_success:
        print("æŸäº›ä¾èµ–åŒ…å®‰è£…å¤±è´¥ï¼Œç¨‹åºå¯èƒ½æ— æ³•æ­£å¸¸è¿è¡Œï¼")
        if input("æ˜¯å¦ç»§ç»­è¿è¡Œï¼Ÿ(y/n): ").lower() != 'y':
            sys.exit(1)
try:
    # æ£€æŸ¥å¹¶å®‰è£…ä¾èµ–åŒ…
    check_and_install_packages()
    
    # å¯¼å…¥æ‰€éœ€çš„åŒ…
    import requests
    import pandas as pd
    import logging
    import time
    from datetime import datetime, timedelta
    import os
    from typing import Dict, List
    from bs4 import BeautifulSoup
    import urllib3
    import json
    from tqdm import tqdm
    import chardet
    
    # ç¦ç”¨SSLè­¦å‘Š
    urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
    
except Exception as e:
    print(f"\nç¨‹åºåˆå§‹åŒ–å¤±è´¥: {str(e)}")
    sys.exit(1)

class MarketCrawler:
    def __init__(self):
        self.base_url = "https://pfsc.agri.cn/api"
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36",
            "Accept": "application/json, text/plain, */*",
            "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
            "Content-Type": "application/json;charset=UTF-8",
            "Origin": "https://pfsc.agri.cn",
            "Referer": "https://pfsc.agri.cn/",
            "Connection": "keep-alive",
            "sec-ch-ua": '"Google Chrome";v="131", "Chromium";v="131", "Not_A Brand";v="24"',
            "sec-ch-ua-mobile": "?0",
            "sec-ch-ua-platform": "Windows",
            "sec-fetch-dest": "empty",
            "sec-fetch-mode": "cors",
            "sec-fetch-site": "same-origin"
        }
        # ä¿®æ”¹æ•°æ®ä¿å­˜ç›®å½•ä¸ºç›¸å¯¹è·¯å¾„
        script_dir = os.path.dirname(os.path.abspath(__file__))
        self.data_dir = os.path.join(script_dir, "market_data")
        os.makedirs(self.data_dir, exist_ok=True)
        
        # çœä»½ä»£ç åˆ—è¡¨
        self.provinces = [
            {"code": "110000", "name": "åŒ—äº¬å¸‚"},
            {"code": "120000", "name": "å¤©æ´¥å¸‚"},
            {"code": "130000", "name": "æ²³åŒ—çœ"},
            {"code": "140000", "name": "å±±è¥¿çœ"},
            {"code": "150000", "name": "å†…è’™å¤è‡ªæ²»åŒº"},
            {"code": "210000", "name": "è¾½å®çœ"},
            {"code": "220000", "name": "å‰æ—çœ"},
            {"code": "230000", "name": "é»‘é¾™æ±Ÿçœ"},
            {"code": "310000", "name": "ä¸Šæµ·å¸‚"},
            {"code": "320000", "name": "æ±Ÿè‹çœ"},
            {"code": "330000", "name": "æµ™æ±Ÿçœ"},
            {"code": "340000", "name": "å®‰å¾½çœ"},
            {"code": "350000", "name": "ç¦å»ºçœ"},
            {"code": "360000", "name": "æ±Ÿè¥¿çœ"},
            {"code": "370000", "name": "å±±ä¸œçœ"},
            {"code": "410000", "name": "æ²³å—çœ"},
            {"code": "420000", "name": "æ¹–åŒ—çœ"},
            {"code": "430000", "name": "æ¹–å—çœ"},
            {"code": "440000", "name": "å¹¿ä¸œçœ"},
            {"code": "450000", "name": "å¹¿è¥¿å£®æ—è‡ªæ²»åŒº"},
            {"code": "460000", "name": "æµ·å—çœ"},
            {"code": "500000", "name": "é‡åº†å¸‚"},
            {"code": "510000", "name": "å››å·çœ"},
            {"code": "520000", "name": "è´µå·çœ"},
            {"code": "530000", "name": "äº‘å—çœ"},
            {"code": "540000", "name": "è¥¿è—è‡ªæ²»åŒº"},
            {"code": "610000", "name": "é™•è¥¿çœ"},
            {"code": "620000", "name": "ç”˜è‚ƒçœ"},
            {"code": "630000", "name": "é’æµ·çœ"},
            {"code": "640000", "name": "å®å¤å›æ—è‡ªæ²»åŒº"},
            {"code": "650000", "name": "æ–°ç–†ç»´å¾å°”è‡ªæ²»åŒº"}
        ]
        
        # æ·»åŠ é…ç½®é€‰é¡¹
        self.config = {
            "retry_times": 3,
            "retry_delay": 5,
            "export_format": "both",  # å¯é€‰: "csv", "json", "both"
        }
        
        self.setup_logging()

    def setup_logging(self):
        # ç¡®ä¿æ—¥å¿—æ–‡ä»¶ä¿å­˜åœ¨è„šæœ¬æ‰€åœ¨ç›®
        script_dir = os.path.dirname(os.path.abspath(__file__))
        log_file = os.path.join(script_dir, 'market_crawler.log')
        
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_file, encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging

    def fetch_provinces(self) -> List[Dict]:
        """è·å–æ‰€æœ‰çœä»½ä¿¡æ¯"""
        try:
            url = f"{self.base_url}/priceQuotationController/getProvinceList"
            response = requests.get(
                url,
                headers=self.headers,
                verify=False,
                timeout=30
            )
            response.raise_for_status()
            data = response.json()
            if data.get("code") == 200:
                return data.get("content", [])
            return []
        except Exception as e:
            self.logger.error(f"è·å–çœä»½åˆ—è¡¨å¤±è´¥: {str(e)}")
            return []

    def get_export_config(self):
        """è·å–å¯¼å‡ºé…ç½®"""
        print("\n=== å¯¼å‡ºé…ç½® ===")
        print("1. ä»…å¯¼å‡ºCSV")
        print("2. ä»…å¯¼å‡ºJSON")
        print("3. ä¸¤ç§æ ¼å¼éƒ½å¯¼å‡º(è®¤)")
        
        while True:
            choice = input("è¯·é€‰æ‹©å¯¼å‡ºæ ¼å¼ [1-3]: ").strip()
            if not choice:  # é»˜è®¤ä¸¤ç§éƒ½å¯¼å‡º
                return
            
            try:
                choice = int(choice)
                if choice == 1:
                    self.config["export_format"] = "csv"
                    break
                elif choice == 2:
                    self.config["export_format"] = "json"
                    break
                elif choice == 3:
                    self.config["export_format"] = "both"
                    break
                else:
                    print("è¯·è¾“å…¥1-3ä¹‹é—´çš„æ•°å­—ï¼")
            except ValueError:
                print("è¯·è¾“å…¥æœ‰æ•ˆçš„æ•°å­—ï¼")
        
        print(f"\nå·²é€‰æ‹©: {self.config['export_format'].upper()}")

    def check_price_changed(self, market_name: str, new_data: List[Dict]) -> bool:
        """æ£€æŸ¥ä»·æ ¼æ˜¯å¦æœ‰å˜åŒ–"""
        try:
            market_dir = os.path.join(self.data_dir, market_name)
            if not os.path.exists(market_dir):
                return True  # ç›®å½•ä¸å­˜åœ¨ï¼Œè¯´æ˜æ˜¯æ–°å¸‚åœº
            
            # è·å–æœ€æ–°çš„CSVæ–‡ä»¶
            csv_files = [f for f in os.listdir(market_dir) if f.endswith('.csv') and not f.endswith('_all.csv')]
            if not csv_files:
                return True  # æ²¡æœ‰å†å²æ–‡ä»¶ï¼Œéœ€è¦ä¿å­˜
            
            latest_file = max(csv_files, key=lambda x: os.path.getctime(os.path.join(market_dir, x)))
            latest_df = pd.read_csv(os.path.join(market_dir, latest_file))
            
            # ç¡®ä¿æ•°æ®æ ¼å¼ä¸€è‡´
            latest_df['äº¤æ˜“æ—¥æœŸ'] = pd.to_datetime(latest_df['äº¤æ˜“æ—¥æœŸ']).dt.date
            new_df = pd.DataFrame(new_data)
            new_df['äº¤æ˜“æ—¥æœŸ'] = pd.to_datetime(new_df['äº¤æ˜“æ—¥æœŸ']).dt.date
            
            # åªæ¯”è¾ƒä»Šå¤©çš„æ•°æ®
            today = datetime.now().date()
            latest_today = latest_df[latest_df['äº¤æ˜“æ—¥æœŸ'] == today]
            new_today = new_df[new_df['äº¤æ˜“æ—¥æœŸ'] == today]
            
            if len(latest_today) == 0 or len(new_today) == 0:
                return True  # ä»»ä¸€æ–¹æ²¡æœ‰ä»Šå¤©çš„æ•°æ®ï¼Œéœ€è¦ä¿å­˜
            
            # æ¯”è¾ƒå…³é”®å­—æ®µæ˜¯å¦æœ‰å˜åŒ–
            price_columns = ['æœ€ä½ä»·', 'å¹³å‡ä»·', 'æœ€é«˜ä»·']
            for col in price_columns:
                if not (latest_today[col] == new_today[col]).all():
                    return True  # ä»·æ ¼æœ‰å˜åŒ–
            
            self.logger.info(f"å¸‚åœº {market_name} ä»·æ ¼æ— å˜åŒ–ï¼Œè·³è¿‡ä¿å­˜")
            return False
            
        except Exception as e:
            self.logger.error(f"æ£€æŸ¥ä»·æ ¼å˜åŒ–å¤±è´¥: {str(e)}")
            return True  # å‡ºé”™æ—¶ä¿é™©èµ·è§è¿˜æ˜¯ä¿å­˜æ•°æ®

    def save_market_data(self, market_name: str, data: List[Dict]):
        """ä¿å­˜å•ä¸ªå¸‚åœºçš„æ•°æ®"""
        try:
            if not data:
                return
            
            # æ£€æŸ¥ä»·æ ¼æ˜¯å¦æœ‰å˜åŒ–
            if not self.check_price_changed(market_name, data):
                return
            
            # ç”Ÿæˆæ–‡ä»¶åå’Œæ—¶é—´æˆ³
            timestamp = datetime.now()
            date_str = timestamp.strftime("%Y%m%d")
            
            # ä¸ºæ•°æ®æ·»åŠ çˆ¬å–æ—¶é—´
            for item in data:
                item['çˆ¬å–æ—¶é—´'] = timestamp.strftime("%Y-%m-%d %H:%M:%S")
            
            # åˆ›å»ºæ—¥æœŸç›®å½•å’Œå¸‚åœºç›®å½•
            date_dir = os.path.join(self.data_dir, date_str)
            safe_market_name = "".join(x for x in market_name if x.isalnum() or x in ['-', '_'])
            market_dir = os.path.join(date_dir, safe_market_name)
            os.makedirs(market_dir, exist_ok=True)
            
            # è½¬æ¢ä¸ºDataFrame
            new_df = pd.DataFrame(data)
            
            # æ ¹æ®é…ç½®ä¿å­˜æ•°æ®
            if self.config["export_format"] in ["json", "both"]:
                json_file = os.path.join(market_dir, f"{safe_market_name}.json")
                if os.path.exists(json_file):
                    # å¦‚æœæ–‡ä»¶å­˜åœ¨ï¼Œè¯»å–å¹¶åˆå¹¶æ•°æ®
                    try:
                        with open(json_file, 'r', encoding='utf-8') as f:
                            existing_data = json.load(f)
                            if isinstance(existing_data, list):
                                # åˆå¹¶æ•°æ®å¹¶å»é‡
                                all_data = existing_data + data
                                # ä½¿ç”¨å­—å…¸å»é‡ï¼Œä¿ç•™æœ€æ–°çš„æ•°æ®
                                unique_data = {}
                                for item in all_data:
                                    key = (item['å¸‚åœºID'], item['å“ç§ID'], item['äº¤æ˜“æ—¥æœŸ'])
                                    unique_data[key] = item
                                data = list(unique_data.values())
                    except Exception as e:
                        self.logger.error(f"è¯»å–JSONæ–‡ä»¶å¤±è´¥: {str(e)}")
                
                # ä¿å­˜åˆå¹¶åçš„æ•°æ®
                with open(json_file, 'w', encoding='utf-8') as f:
                    json.dump(data, f, ensure_ascii=False, indent=2)
                self.logger.info(f"JSONæ•°æ®å·²æ›´æ–°åˆ° {json_file}")
            
            if self.config["export_format"] in ["csv", "both"]:
                csv_file = os.path.join(market_dir, f"{safe_market_name}.csv")
                if os.path.exists(csv_file):
                    # å¦‚æœæ–‡ä»¶å­˜åœ¨ï¼Œè¯»å–å¹¶åˆå¹¶æ•°æ®
                    try:
                        existing_df = pd.read_csv(csv_file)
                        new_df = pd.concat([existing_df, new_df], ignore_index=True)
                        # å»é‡ï¼Œä¿ç•™æœ€æ–°çš„æ•°æ®
                        new_df = new_df.sort_values('çˆ¬å–æ—¶é—´').drop_duplicates(
                            subset=['å¸‚åœºID', 'å“ç§ID', 'äº¤æ˜“æ—¥æœŸ'], 
                            keep='last'
                        )
                    except Exception as e:
                        self.logger.error(f"è¯»å–CSVæ–‡ä»¶å¤±è´¥: {str(e)}")
                
                # ä¿å­˜åˆå¹¶åçš„æ•°æ®
                new_df = new_df.sort_values(['äº¤æ˜“æ—¥æœŸ', 'å“ç§åç§°'])
                new_df.to_csv(csv_file, index=False, encoding='utf-8-sig')
                self.logger.info(f"CSVæ•°æ®å·²æ›´æ–°åˆ° {csv_file}")
                
                # æ›´æ–°æ±‡æ€»æ–‡ä»¶
                summary_file = os.path.join(market_dir, f"{safe_market_name}_summary.csv")
                try:
                    if os.path.exists(summary_file):
                        summary_df = pd.read_csv(summary_file)
                        summary_df = pd.concat([summary_df, new_df], ignore_index=True)
                    else:
                        summary_df = new_df.copy()
                    
                    # å»é‡å¹¶æ’åº
                    summary_df = summary_df.sort_values('çˆ¬å–æ—¶é—´').drop_duplicates(
                        subset=['å¸‚åœºID', 'å“ç§ID', 'äº¤æ˜“æ—¥æœŸ'], 
                        keep='last'
                    )
                    summary_df = summary_df.sort_values(['äº¤æ˜“æ—¥æœŸ', 'å“ç§åç§°'])
                    summary_df.to_csv(summary_file, index=False, encoding='utf-8-sig')
                    self.logger.info(f"æ±‡æ€»æ•°æ®å·²æ›´æ–°åˆ° {summary_file}")
                    
                except Exception as e:
                    self.logger.error(f"æ›´æ–°æ±‡æ€»æ–‡ä»¶å¤±è´¥: {str(e)}")
            
        except Exception as e:
            self.logger.error(f"ä¿å­˜å¸‚åœº {market_name} æ•°æ®å¤±è´¥: {str(e)}")
            raise

    def fetch_market_details(self, market_id: str) -> List[Dict]:
        """è·å–å•ä¸ªå¸‚åœºçš„è¯¦ç»†ä¿¡æ¯"""
        max_retries = 3
        retry_delay = 5
        all_items = []
        page_num = 1
        timestamp = datetime.now()
        
        while True:
            for retry in range(max_retries):
                try:
                    url = f"{self.base_url}/priceQuotationController/pageList"
                    
                    # æ„å»ºè¯·æ±‚ä½“
                    payload = {
                        "marketId": market_id,
                        "pageNum": page_num,
                        "pageSize": 40,
                        "order": "desc",
                        "key": "",
                        "varietyTypeId": "",
                        "varietyId": "",
                        "startDate": (datetime.now() - timedelta(days=1)).strftime("%Y-%m-%d"),
                        "endDate": datetime.now().strftime("%Y-%m-%d")
                    }
                    
                    # å‘é€è¯·æ±‚
                    session = requests.Session()
                    response = session.post(
                        url,
                        headers=self.headers,
                        json=payload,  # ä½¿ç”¨jsonå‚æ•°è€Œä¸æ˜¯data
                        verify=False,
                        timeout=30
                    )
                    
                    # æ£€æŸ¥å“åº”çŠ¶æ€
                    if response.status_code != 200:
                        raise requests.exceptions.RequestException(f"HTTP {response.status_code}")
                    
                    # æ£€æŸ¥å“åº”å†…å®¹
                    if not response.content:
                        raise ValueError("Empty response received")
                    
                    data = response.json()
                    
                    if data.get("code") == 200:
                        content = data.get("content", {})
                        items = content.get("list", [])
                        total = content.get("total", 0)
                        pages = content.get("pages", 1)
                        
                        if items:
                            processed_items = []
                            for item in items:
                                try:
                                    # ä»è¿”å›çš„æ•°æ®ä¸­è·å–çœä»½ä¿¡æ¯
                                    province_name = str(item.get("provinceName", ""))
                                    province_code = str(item.get("provinceCode", ""))
                                    
                                    processed_item = {
                                        # å¸‚åœºåŸºæœ¬ä¿¡æ¯
                                        "å¸‚åœºID": str(market_id),
                                        "å¸‚åœºä»£ç ": str(item.get("marketCode", "")),
                                        "å¸‚åœºåç§°": str(item.get("marketName", "")),
                                        "å¸‚åœºç±»å‹": str(item.get("marketType", "")),
                                        
                                        # å“ç§ä¿¡æ¯
                                        "å“ç§ID": str(item.get("varietyId", "")),
                                        "å“ç§åç§°": str(item.get("varietyName", "")),
                                        
                                        # ä»·æ ¼ä¿¡æ¯
                                        "æœ€ä½ä»·": float(item.get("minimumPrice", 0) or 0),
                                        "å¹³å‡ä»·": float(item.get("middlePrice", 0) or 0),
                                        "æœ€é«˜ä»·": float(item.get("highestPrice", 0) or 0),
                                        "è®¡é‡å•ä½": str(item.get("meteringUnit", "")),
                                        
                                        # äº¤æ˜“ä¿¡æ¯
                                        "äº¤æ˜“æ—¥æœŸ": str(item.get("reportTime", "")),  # ä½¿ç”¨reportTimeä½œä¸ºäº¤æ˜“æ—¥æœŸ
                                        "äº¤æ˜“é‡": float(item.get("tradingVolume", 0) or 0),
                                        
                                        # åœ°ç†ä¿¡æ¯
                                        "äº§åœ°": str(item.get("producePlace", "")),
                                        "é”€å”®åœ°": str(item.get("salePlace", "")),
                                        "çœä»½": province_name,
                                        "çœä»½ä»£ç ": province_code,
                                        "åœ°åŒºåç§°": str(item.get("areaName", "")),  # æ·»åŠ åœ°åŒºä¿¡æ¯
                                        "åœ°åŒºä»£ç ": str(item.get("areaCode", "")),
                                        
                                        # å…¶ä»–ä¿¡æ¯
                                        "å“ç§ç±»å‹": str(item.get("varietyTypeName", "")),
                                        "å“ç§ç±»å‹ID": str(item.get("varietyTypeId", "")),
                                        "å…¥åº“æ—¶é—´": str(item.get("inStorageTime", "")),
                                        "çˆ¬å–æ—¶é—´": timestamp.strftime("%Y-%m-%d %H:%M:%S")
                                    }
                                    
                                    # éªŒè¯å¿…è¦å­—æ®µ
                                    if processed_item["å¸‚åœºåç§°"] and processed_item["äº¤æ˜“æ—¥æœŸ"]:
                                        processed_items.append(processed_item)
                                        
                                except Exception as e:
                                    self.logger.error(f"å¤„ç†æ•°æ®é¡¹å¤±è´¥: {str(e)}, æ•°æ®: {item}")
                                    continue
                            
                            all_items.extend(processed_items)
                            self.logger.info(f"è·å–å¸‚åœº {market_id} ç¬¬ {page_num}/{pages} é¡µæ•°æ®ï¼Œæœ¬é¡µ {len(processed_items)} æ¡")
                            
                            # åˆ¤æ–­æ˜¯å¦è¿˜æœ‰ä¸‹ä¸€é¡µ
                            if page_num >= pages:
                                return all_items  # åˆ°è¾¾æœ€åä¸€é¡µï¼Œè¿”å›æ‰€æœ‰æ•°æ®
                            page_num += 1
                            time.sleep(1)  # ç¿»é¡µé—´éš”
                        else:
                            return all_items  # æ²¡æœ‰æ•°æ®äº†å°±è¿”å›
                            
                        break  # æˆåŠŸè·å–æ•°æ®åè·³å‡ºé‡è¯•å¾ªç¯
                    
                    else:
                        error_msg = data.get("message", "Unknown error")
                        self.logger.error(f"APIè¿”å›é”™è¯¯: {error_msg}")
                        if retry < max_retries - 1:
                            time.sleep(retry_delay)
                            continue
                        return all_items
                    
                except (requests.exceptions.RequestException, json.JSONDecodeError) as e:
                    self.logger.error(f"è¯·æ±‚å¤±è´¥ (é‡è¯• {retry + 1}/{max_retries}): {str(e)}")
                    if retry < max_retries - 1:
                        time.sleep(retry_delay)
                        continue
                    return all_items
                
        return all_items

    def save_summary_data(self):
        """ç”Ÿæˆæ±‡æ€»æ•°æ®"""
        try:
            print("\n=== ç”Ÿæˆæ•°æ®æ±‡æ€» ===")
            all_data = []
            
            # éå†æ—¥æœŸç›®å½•
            for date_dir in os.listdir(self.data_dir):
                date_path = os.path.join(self.data_dir, date_dir)
                if not os.path.isdir(date_path) or date_dir == 'summary':
                    continue
                
                # éå†å¸‚åœºç›®å½•
                for market_dir in os.listdir(date_path):
                    market_path = os.path.join(date_path, market_dir)
                    if not os.path.isdir(market_path):
                        continue
                    
                    # è¯»å–è¯¥å¸‚åœºçš„æ‰€æœ‰CSVæ–‡ä»¶
                    csv_files = [f for f in os.listdir(market_path) if f.endswith('.csv')]
                    
                    for csv_file in csv_files:
                        try:
                            df = pd.read_csv(os.path.join(market_path, csv_file))
                            all_data.append(df)
                        except Exception as e:
                            self.logger.error(f"è¯»å–æ–‡ä»¶ {csv_file} å¤±è´¥: {str(e)}")
            
            if all_data:
                # åˆå¹¶æ‰€æœ‰æ•°æ®
                summary_df = pd.concat(all_data, ignore_index=True)
                
                # ç¡®ä¿æ—¥æœŸæ ¼å¼æ­£ç¡®
                summary_df['äº¤æ˜“æ—¥æœŸ'] = pd.to_datetime(summary_df['äº¤æ˜“æ—¥æœŸ'])
                summary_df['çˆ¬å–æ—¶é—´'] = pd.to_datetime(summary_df['çˆ¬å–æ—¶é—´'])
                
                # åˆ›å»ºæ±‡æ€»ç›®å½•
                summary_dir = os.path.join(self.data_dir, 'summary')
                os.makedirs(summary_dir, exist_ok=True)
                
                # æŒ‰æ—¥æœŸåˆ†ç»„ä¿å­˜
                for date, group in summary_df.groupby(summary_df['äº¤æ˜“æ—¥æœŸ'].dt.date):
                    date_str = date.strftime('%Y%m%d')
                    
                    # ä¿å­˜CSV
                    csv_file = os.path.join(summary_dir, f'summary_{date_str}.csv')
                    group.to_csv(csv_file, index=False, encoding='utf-8-sig')
                    print(f"âœ“ å·²ç”Ÿæˆ {date_str} çš„CSVæ±‡æ€»æ•°æ®")
                    
                    # ä¿å­˜JSON
                    json_file = os.path.join(summary_dir, f'summary_{date_str}.json')
                    group.to_json(json_file, orient='records', force_ascii=False, indent=2)
                    print(f"âœ“ å·²ç”Ÿæˆ {date_str} çš„JSONæ±‡æ€»æ•°æ®")
                
                # ç”Ÿæˆå®Œæ•´æ±‡æ€»
                summary_df = summary_df.sort_values(['äº¤æ˜“æ—¥æœŸ', 'çœä»½', 'å¸‚åœºåç§°', 'å“ç§åç§°'])
                
                # ä¿å­˜å®Œæ•´æ±‡æ€»æ–‡ä»¶
                all_csv = os.path.join(summary_dir, 'summary_all.csv')
                summary_df.to_csv(all_csv, index=False, encoding='utf-8-sig')
                print(f"\nâœ“ ç”Ÿæˆå®Œæ•´æ±‡æ€»CSV: {all_csv}")
                
                all_json = os.path.join(summary_dir, 'summary_all.json')
                summary_df.to_json(all_json, orient='records', force_ascii=False, indent=2)
                print(f"âœ“ å·²ç”Ÿæˆå®Œæ•´æ±‡æ€»JSON: {all_json}")
                
                # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
                print("\n=== æ•°æ®ç»Ÿè®¡ ===")
                print(f"æ€»è®°å½•æ•°: {len(summary_df)}")
                print(f"è¦†ç›–æ—¥æœŸ: {summary_df['äº¤æ˜“æ—¥æœŸ'].min():%Y-%m-%d} è‡³ {summary_df['äº¤æ˜“æ—¥æœŸ'].max():%Y-%m-%d}")
                print(f"çœä»½æ•°é‡: {summary_df['çœä»½'].nunique()}")
                print(f"å¸‚åœºæ•°é‡: {summary_df['å¸‚åœºåç§°'].nunique()}")
                print(f"å“ç§æ•°é‡: {summary_df['å“ç§åç§°'].nunique()}")
                
            else:
                print("æ²¡æœ‰æ‰¾åˆ°å¯ç”¨çš„æ•°æ®æ–‡ä»¶ï¼")
                
        except Exception as e:
            self.logger.error(f"ç”Ÿæˆæ±‡æ€»æ•°æ®å¤±è´¥: {str(e)}")
            raise

    def merge_json_files(self):
        """åˆå¹¶æ‰€æœ‰JSONæ–‡ä»¶åˆ°ä¸€ä¸ªæ€»æ–‡ä»¶"""
        try:
            print("\n=== åˆå¹¶JSONæ–‡ä»¶ ===")
            all_data = []
            
            # éå†æ—¥æœŸç›®å½•
            for date_dir in os.listdir(self.data_dir):
                date_path = os.path.join(self.data_dir, date_dir)
                if not os.path.isdir(date_path):
                    continue
                    
                # éå†å¸‚åœºç›®å½•
                for market_dir in os.listdir(date_path):
                    market_path = os.path.join(date_path, market_dir)
                    if not os.path.isdir(market_path):
                        continue
                        
                    # è·å–æ‰€æœ‰JSONæ–‡ä»¶
                    json_files = [f for f in os.listdir(market_path) if f.endswith('.json')]
                    
                    for json_file in json_files:
                        try:
                            with open(os.path.join(market_path, json_file), 'r', encoding='utf-8') as f:
                                data = json.load(f)
                                if isinstance(data, list):
                                    all_data.extend(data)
                                else:
                                    all_data.append(data)
                        except Exception as e:
                            self.logger.error(f"è¯»å–JSONæ–‡ä»¶ {json_file} å¤±: {str(e)}")
            
            if all_data:
                # æŒ‰æ—¥æœŸæ’åº
                all_data.sort(key=lambda x: (x.get('äº¤æ˜“æ—¥æœŸ', ''), x.get('çœä»½', ''), x.get('å¸‚åœºåç§°', '')))
                
                # åˆ›å»ºmergedç›®å½•
                merged_dir = os.path.join(self.data_dir, 'merged')
                os.makedirs(merged_dir, exist_ok=True)
                
                # ä¿å­˜åˆå¹¶åçš„JSONæ–‡ä»¶
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                merged_file = os.path.join(merged_dir, f'all_markets_{timestamp}.json')
                
                with open(merged_file, 'w', encoding='utf-8') as f:
                    json.dump(all_data, f, ensure_ascii=False, indent=2)
                
                print(f"\nâœ“ å·²ç”Ÿæˆåˆå¹¶JSONæ–‡ä»¶: {merged_file}")
                print(f"  - è®°å½•æ•°: {len(all_data)}")
                print(f"  - å¸‚åœºæ•°é‡: {len(set(item.get('å¸‚åœºåç§°', '') for item in all_data))}")
                print(f"  - çœä»½æ•°é‡: {len(set(item.get('çœä»½', '') for item in all_data))}")
                
                # åŒæ—¶ç”ŸæˆCSVç‰ˆæœ¬
                df = pd.DataFrame(all_data)
                csv_file = merged_file.replace('.json', '.csv')
                df.to_csv(csv_file, index=False, encoding='utf-8-sig')
                print(f"âœ“ å·²ç”Ÿæˆåˆå¹¶CSVæ–‡ä»¶: {csv_file}")
                
            else:
                print("æ²¡æœ‰æ‰¾åˆ°å¯ç”¨çš„JSONæ–‡ä»¶ï¼")
                
        except Exception as e:
            self.logger.error(f"åˆå¹¶JSONæ–‡ä»¶å¤±è´¥: {str(e)}")
            raise

    def run(self, interval_minutes: int = 30):
        """è¿è¡Œçˆ¬è™«ï¼Œå®šæœŸè·å–æ•°æ®"""
        self.logger.info("å¼€å§‹è¿è¡Œå¸‚åœºæ•°æ®çˆ¬è™«...")
        
        # è·å–å¯¼å‡ºé…ç½®
        self.get_export_config()
        
        # è¯¢é—®æ˜¯å¦éœ€è¦åˆå¹¶å†å²JSONæ–‡ä»¶
        merge_choice = input("\næ˜¯å¦åˆå¹¶å†å²JSONæ–‡ä»¶ï¼Ÿ(y/nï¼Œé»˜è®¤n): ").strip().lower()
        if merge_choice == 'y':
            self.merge_json_files()
            if input("\næ˜¯å¦ç»§ç»­çˆ¬å–æ–°æ•°æ®ï¼Ÿ(y/nï¼Œé»˜è®¤y): ").strip().lower() == 'n':
                return
        
        # æ˜¾ç¤ºæ‰€æœ‰å¯é€‰çœä»½
        print("\n=== çœä»½é€‰æ‹© ===")
        for i, province in enumerate(self.provinces, 1):
            print(f"{i}. {province['name']}")
        
        # è·å–ç”¨æˆ·è¾“å…¥
        while True:
            try:
                choices = input("\nè¯·è¾“å…¥è¦çˆ¬å–çš„çœä»½åºå·(å¤šä¸ªçœä»½ç”¨å·åˆ†éš”ï¼Œç›´æ¥å›è½¦çˆ¬å–æ‰€æœ‰çœä»½): ").strip()
                if not choices:  # ç›´æ¥å›è½¦ï¼Œçˆ¬å–æ‰€æœ‰çœä»½
                    selected_provinces = self.provinces
                    break
                
                # è§£æç”¨æˆ·è¾“
                indices = [int(x.strip()) for x in choices.split(",")]
                # éªŒè¯è¾“å…¥çš„åºå·æ˜¯å¦æœ‰æ•ˆ
                if all(1 <= i <= len(self.provinces) for i in indices):
                    selected_provinces = [self.provinces[i-1] for i in indices]
                    break
                else:
                    print("è¾“å…¥åºå·æ— æ•ˆï¼Œè¯·é‡æ–°è¾“å…¥ï¼")
            except ValueError:
                print("è¾“æ ¼å¼é”™è¯¯ï¼Œè¯·è¾“å…¥æ•°å­—åºå·ï¼Œå¤šä¸ªåºå·ç”¨é€—å·åˆ†éš”ï¼")
        
        # æ˜¾ç¤ºé€‰æ‹©çš„çœä»½
        print("\nå·²é€‰æ‹©ä»¥ä¸‹çœä»½:")
        for province in selected_provinces:
            print(f"- {province['name']}")
        
        while True:
            try:
                data_changed = False  # æ ‡è®°æ˜¯å¦æœ‰æ•°æ®å˜åŒ–
                
                # éå†é€‰å®šçš„çœä»½
                for province in selected_provinces:
                    province_code = province["code"]
                    province_name = province["name"]
                    
                    self.logger.info(f"å¼€å§‹è·å–{province_name}çš„å¸‚åœºæ•°æ®...")
                    
                    # è·å–è¯¥çœä»½çš„æ‰€æœ‰å¸‚åœº
                    url = f"{self.base_url}/priceQuotationController/getTodayMarketByProvinceCode"
                    params = {"code": province_code}
                    
                    response = requests.post(
                        url, 
                        headers=self.headers, 
                        params=params,
                        verify=False,
                        timeout=30
                    )
                    response.raise_for_status()
                    
                    data = response.json()
                    if data["code"] == 200 and "content" in data:
                        markets = data["content"]
                        
                        self.logger.info(f"{province_name}å…±æœ‰ {len(markets)} ä¸ªå¸‚åœº")
                        
                        for market in markets:
                            market_id = market.get("marketId")
                            market_name = market.get("marketName")
                            
                            if market_id and market_name:
                                details = self.fetch_market_details(market_id)
                                if details:
                                    # æ·»åŠ çœä»½ä¿¡æ¯
                                    for detail in details:
                                        detail["çœä»½"] = province_name
                                        detail["çœä»½ä»£ç "] = province_code
                                    
                                    # ä¿å­˜è¯¥å¸‚åœºæ•°æ®ï¼ˆå¦‚æœæœ‰å˜åŒ–ï¼‰
                                    if self.check_price_changed(market_name, details):
                                        self.save_market_data(market_name, details)
                                        self.logger.info(f"æˆåŠŸè·å–å¹¶ä¿å­˜ {market_name} çš„ {len(details)} æ¡æ•°æ®")
                                        data_changed = True
                                    
                                time.sleep(2)
                
                    # å¤„ç†ä¸€ä¸ªçœä»½åç¨ä½œç­‰å¾…
                    time.sleep(5)
                
                # åªæœ‰åœ¨æ•°æ®æœ‰å˜åŒ–æ—¶æ‰ç”Ÿæˆæ±‡æ€»
                if data_changed:
                    self.save_summary_data()
                else:
                    self.logger.info("æœ¬è½®æ‰€æœ‰å¸‚åœºä»·æ ¼æ— å˜åŒ–")
                
                self.logger.info(f"æœ¬è½®æ•°æ®è·å–å®Œæˆï¼Œç­‰å¾… {interval_minutes} åˆ†é’Ÿåè¿›è¡Œä¸‹ä¸€æ¬¡è·å–...")
                time.sleep(interval_minutes * 60)
                
            except Exception as e:
                self.logger.error(f"è¿è¡Œå‡ºé”™: {str(e)}")
                time.sleep(60)

def api_mode():
    """APIæ¨¡å¼ä¸‹çš„æ•°æ®è·å–"""
    crawler = MarketCrawler()
    data = []
    
    try:
        # è·å–æœ€æ–°çš„å¸‚åœºæ•°æ®
        for province in crawler.provinces:
            province_code = province["code"]
            markets = crawler.fetch_market_details(province_code)
            if markets:
                data.extend(markets)
        
        # è¾“å‡ºJSONæ ¼å¼æ•°æ®
        print(json.dumps(data, ensure_ascii=False))
        return 0
    except Exception as e:
        print(json.dumps({
            "error": str(e)
        }, ensure_ascii=False))
        return 1

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument('--mode', default='normal', 
                      choices=['normal', 'api'],
                      help='è¿è¡Œæ¨¡å¼: normal=æ™®é€šæ¨¡å¼, api=APIæ¨¡å¼')
    args = parser.parse_args()
    
    if args.mode == 'api':
        sys.exit(api_mode())
    else:
        crawler = MarketCrawler()
        crawler.run(interval_minutes=30) 
    
    # æœ€åä¸€æ­¥ï¼Œç”Ÿæˆæ±‡æ€»æ•°æ®
        crawler.save_summary_data()
        crawler.merge_json_files()
        crawler.logger.info("æ•°æ®çˆ¬å–å®Œæˆï¼")

